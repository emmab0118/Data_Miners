{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f7adf6",
   "metadata": {},
   "source": [
    "want to try and optimize feature selection\n",
    "\n",
    "from what i can see, accident, vehicle, and person datasets seem the most relevant, might try to take some features from there\n",
    "\n",
    "converting each csv into a dataframe for better handling\n",
    "- df is our base df, will be writing it to a new csv when done\n",
    "- finding intersection btwn all three csvs, using that to drop features from df1 and df2.\n",
    "- i then will be adding df1 and df2 (with only unique cols) into df\n",
    "- from here, determine features with strongest correlation to target since our max is 20 features\n",
    "- going to run decision tree model from demo.ipynb on new csv to see if any improvements in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef7e32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_25672\\4190137545.py:12: DtypeWarning: Columns (57,59,69,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\vehicle23.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "RANDOM_STATE = 42\n",
    "files = [\"accident23\", \"person23\", \"vehicle23\"]\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\accident23.csv\")\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\vehicle23.csv\")\n",
    "df2 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\person23.csv\")\n",
    "\n",
    "df3 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d257ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50103\n",
      "87461\n",
      "122388\n"
     ]
    }
   ],
   "source": [
    "accidents_col = list(df.columns)\n",
    "vehicles_col = list(df1.columns)\n",
    "person_col = list(df2.columns)\n",
    "\n",
    "intersect = []\n",
    "\n",
    "for x in accidents_col: #there has to be a better way to do this.. \n",
    "    if x in vehicles_col:\n",
    "        if x in person_col:\n",
    "            intersect.append(x) #if feature is in all 3 dfs\n",
    "\n",
    "intersect.remove(\"CASENUM\") #we wanna keep casenum since this is our joining feature\n",
    "#gonna use this list to drop features from df1 and df2\n",
    "\n",
    "df1 = df1.drop(columns = intersect, errors = \"ignore\")\n",
    "df2 = df2.drop(columns = intersect, errors = \"ignore\")\n",
    "\n",
    "print(len(df))\n",
    "print(len(df1))\n",
    "print(len(df2)) #each file has a differnt num of entries, we need to match case numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad91379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CASENUM', 'PSU', 'PSU_VAR', 'PSUSTRAT', 'REGION', 'REGIONNAME', 'URBANICITY', 'URBANICITYNAME', 'STRATUM', 'STRATUMNAME', 'PJ', 'WEIGHT', 'PEDS', 'PERNOTMVIT', 'VE_TOTAL', 'VE_FORMS', 'PVH_INVL', 'PERMVIT', 'MONTH', 'MONTHNAME', 'DAY_WEEK', 'DAY_WEEKNAME', 'WKDY_IM', 'WKDY_IMNAME', 'YEAR', 'YEARNAME', 'HOUR', 'HOURNAME', 'HOUR_IM', 'HOUR_IMNAME', 'MINUTE', 'MINUTENAME', 'MINUTE_IM', 'MINUTE_IMNAME', 'HARM_EV', 'HARM_EVNAME', 'EVENT1_IM', 'EVENT1_IMNAME', 'MAN_COLL', 'MAN_COLLNAME', 'MANCOL_IM', 'MANCOL_IMNAME', 'RELJCT1', 'RELJCT1NAME', 'RELJCT1_IM', 'RELJCT1_IMNAME', 'RELJCT2', 'RELJCT2NAME', 'RELJCT2_IM', 'RELJCT2_IMNAME', 'TYP_INT', 'TYP_INTNAME', 'REL_ROAD', 'REL_ROADNAME', 'WRK_ZONE', 'WRK_ZONENAME', 'LGT_COND', 'LGT_CONDNAME', 'LGTCON_IM', 'LGTCON_IMNAME', 'WEATHER', 'WEATHERNAME', 'WEATHR_IM', 'WEATHR_IMNAME', 'SCH_BUS_x', 'SCH_BUSNAME_x', 'INT_HWY', 'INT_HWYNAME', 'MAX_SEV', 'MAX_SEVNAME', 'MAXSEV_IM', 'MAXSEV_IMNAME', 'NUM_INJ', 'NUM_INJNAME', 'NO_INJ_IM', 'NO_INJ_IMNAME', 'ALCOHOL', 'ALCOHOLNAME', 'ALCHL_IM', 'ALCHL_IMNAME', 'VEH_NO_x', 'NUMOCCS', 'NUMOCCSNAME', 'UNITTYPE', 'UNITTYPENAME', 'HIT_RUN', 'HIT_RUNNAME', 'VIN', 'VINNAME', 'MOD_YEAR_x', 'MOD_YEARNAME_x', 'MDLYR_IM', 'MDLYR_IMNAME', 'VPICMAKE_x', 'VPICMAKENAME_x', 'VPICMODEL_x', 'VPICMODELNAME_x', 'VPICBODYCLASS_x', 'VPICBODYCLASSNAME_x', 'MAKE_x', 'MAKENAME_x', 'MODEL', 'BODY_TYP_x', 'BODY_TYPNAME_x', 'ICFINALBODY_x', 'ICFINALBODYNAME_x', 'GVWR_FROM_x', 'GVWR_FROMNAME_x', 'GVWR_TO_x', 'GVWR_TONAME_x', 'TOW_VEH_x', 'TOW_VEHNAME_x', 'TRLR1VIN', 'TRLR1VINNAME', 'TRLR2VIN', 'TRLR2VINNAME', 'TRLR3VIN', 'TRLR3VINNAME', 'TRLR1GVWR', 'TRLR1GVWRNAME', 'TRLR2GVWR', 'TRLR2GVWRNAME', 'TRLR3GVWR', 'TRLR3GVWRNAME', 'J_KNIFE', 'J_KNIFENAME', 'MCARR_ID', 'MCARR_IDNAME', 'MCARR_I1', 'MCARR_I1NAME', 'MCARR_I2', 'MCARR_I2NAME', 'V_Config', 'V_ConfigNAME', 'CARGO_BT', 'CARGO_BTNAME', 'HAZ_INV', 'HAZ_INVNAME', 'HAZ_PLAC', 'HAZ_PLACNAME', 'HAZ_ID', 'HAZ_IDNAME', 'HAZ_CNO', 'HAZ_CNONAME', 'HAZ_REL', 'HAZ_RELNAME', 'BUS_USE', 'BUS_USENAME', 'SPEC_USE_x', 'SPEC_USENAME_x', 'EMER_USE_x', 'EMER_USENAME_x', 'TRAV_SP', 'TRAV_SPNAME', 'UNDEROVERRIDE', 'UNDEROVERRIDENAME', 'ROLLOVER_x', 'ROLLOVERNAME_x', 'ROLINLOC', 'ROLINLOCNAME', 'IMPACT1_x', 'IMPACT1NAME_x', 'IMPACT1_IM', 'IMPACT1_IMNAME', 'DEFORMED', 'DEFORMEDNAME', 'TOWED', 'TOWEDNAME', 'M_HARM', 'M_HARMNAME', 'VEVENT_IM', 'VEVENT_IMNAME', 'FIRE_EXP_x', 'FIRE_EXPNAME_x', 'DR_PRES', 'DR_PRESNAME', 'MAX_VSEV', 'MAX_VSEVNAME', 'MXVSEV_IM', 'MXVSEV_IMNAME', 'NUM_INJV', 'NUM_INJVNAME', 'NUMINJ_IM', 'NUMINJ_IMNAME', 'VEH_ALCH', 'VEH_ALCHNAME', 'V_ALCH_IM', 'V_ALCH_IMNAME', 'MAK_MOD_x', 'MAK_MODNAME_x', 'DR_ZIP', 'DR_ZIPNAME', 'SPEEDREL', 'SPEEDRELNAME', 'VTRAFWAY', 'VTRAFWAYNAME', 'VNUM_LAN', 'VNUM_LANNAME', 'VSPD_LIM', 'VSPD_LIMNAME', 'VALIGN', 'VALIGNNAME', 'VPROFILE', 'VPROFILENAME', 'VSURCOND', 'VSURCONDNAME', 'VTRAFCON', 'VTRAFCONNAME', 'VTCONT_F', 'VTCONT_FNAME', 'P_CRASH1', 'P_CRASH1NAME', 'PCRASH1_IM', 'PCRASH1_IMNAME', 'P_CRASH2', 'P_CRASH2NAME', 'P_CRASH3', 'P_CRASH3NAME', 'PCRASH4', 'PCRASH4NAME', 'PCRASH5', 'PCRASH5NAME', 'ACC_TYPE', 'ACC_TYPENAME', 'ACC_CONFIG', 'ACC_CONFIGNAME', 'VEH_NO_y', 'PER_NO', 'SCH_BUS_y', 'SCH_BUSNAME_y', 'MOD_YEAR_y', 'MOD_YEARNAME_y', 'VPICMAKE_y', 'VPICMAKENAME_y', 'VPICMODEL_y', 'VPICMODELNAME_y', 'VPICBODYCLASS_y', 'VPICBODYCLASSNAME_y', 'MAKE_y', 'MAKENAME_y', 'BODY_TYP_y', 'BODY_TYPNAME_y', 'ICFINALBODY_y', 'ICFINALBODYNAME_y', 'GVWR_FROM_y', 'GVWR_FROMNAME_y', 'GVWR_TO_y', 'GVWR_TONAME_y', 'TOW_VEH_y', 'TOW_VEHNAME_y', 'SPEC_USE_y', 'SPEC_USENAME_y', 'EMER_USE_y', 'EMER_USENAME_y', 'ROLLOVER_y', 'ROLLOVERNAME_y', 'IMPACT1_y', 'IMPACT1NAME_y', 'FIRE_EXP_y', 'FIRE_EXPNAME_y', 'MAK_MOD_y', 'MAK_MODNAME_y', 'AGE', 'AGENAME', 'AGE_IM', 'AGE_IMNAME', 'SEX', 'SEXNAME', 'SEX_IM', 'SEX_IMNAME', 'PER_TYP', 'PER_TYPNAME', 'DEVTYPE', 'DEVTYPENAME', 'DEVMOTOR', 'DEVMOTORNAME', 'INJ_SEV', 'INJ_SEVNAME', 'INJSEV_IM', 'INJSEV_IMNAME', 'SEAT_POS', 'SEAT_POSNAME', 'SEAT_IM', 'SEAT_IMNAME', 'REST_USE', 'REST_USENAME', 'REST_MIS', 'REST_MISNAME', 'HELM_USE', 'HELM_USENAME', 'HELM_MIS', 'HELM_MISNAME', 'AIR_BAG', 'AIR_BAGNAME', 'EJECTION', 'EJECTIONNAME', 'EJECT_IM', 'EJECT_IMNAME', 'DRINKING', 'DRINKINGNAME', 'PERALCH_IM', 'PERALCH_IMNAME', 'ALC_STATUS', 'ALC_STATUSNAME', 'ATST_TYP', 'ATST_TYPNAME', 'ALC_RES', 'ALC_RESNAME', 'DRUGS', 'DRUGSNAME', 'HOSPITAL', 'HOSPITALNAME', 'STR_VEH', 'LOCATION', 'LOCATIONNAME']\n",
      "49063\n"
     ]
    }
   ],
   "source": [
    "#merge all three dfs based on casenum\n",
    "\n",
    "#when i first ran all my code the df len was 500,000\n",
    "#assuming this is because of multiple people per car in accident?\n",
    "# if we remove these two lines then we will have much more data to work with\n",
    "df1_first = df1.groupby(\"CASENUM\").head(1)\n",
    "df2_first = df2.groupby(\"CASENUM\").head(1)\n",
    "\n",
    "df3 = (\n",
    "    df\n",
    "    .merge(df1_first, on=\"CASENUM\", how=\"left\")\n",
    "    .merge(df2_first, on=\"CASENUM\", how=\"left\")\n",
    ")\n",
    "\n",
    "\"\"\"df3 = (\n",
    "    df\n",
    "    .merge(df1, on=\"CASENUM\", how=\"left\")\n",
    "    .merge(df2, on=\"CASENUM\", how=\"left\")\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "df3 = df3[df3['MAX_SEV'] <= 4] #drop where sev vals > 4\n",
    "print(list(df3.columns))\n",
    "\n",
    "#so by this point what we have is df3, which is a dataframe consisting of\n",
    "#each unique column in the three csvs\n",
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05d182ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        column                                      unknown_codes  \\\n",
      "11    VSPD_LIM                                           [98, 99]   \n",
      "20    REST_USE                                           [98, 99]   \n",
      "16         AGE                                         [998, 999]   \n",
      "15    DEFORMED                                             [8, 9]   \n",
      "17         SEX                                             [8, 9]   \n",
      "14    SPEEDREL                                             [8, 9]   \n",
      "3      WEATHER                                           [98, 99]   \n",
      "13    ACC_TYPE  [5, 10, 16, 33, 43, 49, 53, 63, 67, 75, 85, 91...   \n",
      "2         HOUR                                               [99]   \n",
      "4     LGT_COND                                             [8, 9]   \n",
      "6     MAN_COLL                                           [98, 99]   \n",
      "8      TYP_INT                                           [98, 99]   \n",
      "12    VSURCOND                                               [99]   \n",
      "7     REL_ROAD                                           [98, 99]   \n",
      "18     PER_TYP                                            [9, 19]   \n",
      "19    SEAT_POS                                           [98, 99]   \n",
      "5      HARM_EV                                               [99]   \n",
      "0        MONTH                                                 []   \n",
      "1     DAY_WEEK                                                [9]   \n",
      "9   URBANICITY                                                 []   \n",
      "10     MAX_SEV                                                 []   \n",
      "\n",
      "    unknown_count  unknown_percent  \n",
      "11           7863           16.026  \n",
      "20           5246           10.692  \n",
      "16           3279            6.683  \n",
      "15           2783            5.672  \n",
      "17           2393            4.877  \n",
      "14           1764            3.595  \n",
      "3            1416            2.886  \n",
      "13           1018            2.075  \n",
      "2             254            0.518  \n",
      "4             144            0.294  \n",
      "6             117            0.238  \n",
      "8              99            0.202  \n",
      "12             90            0.183  \n",
      "7              23            0.047  \n",
      "18              9            0.018  \n",
      "19              7            0.014  \n",
      "5               6            0.012  \n",
      "0               0            0.000  \n",
      "1               0            0.000  \n",
      "9               0            0.000  \n",
      "10              0            0.000  \n"
     ]
    }
   ],
   "source": [
    "#checking for missing/unknown values and changing to NaN\n",
    "#most of the code taken from demo.ipynb\n",
    "\n",
    "#tentative colnames from each csv\n",
    "acc_cols = [\n",
    "    \"MONTH\",\n",
    "    \"DAY_WEEK\",\n",
    "    \"HOUR\",\n",
    "    \"WEATHER\",\n",
    "    \"LGT_COND\",\n",
    "    \"HARM_EV\", #first harmful event that took place\n",
    "    \"MAN_COLL\", #the kind of collision\n",
    "    \"REL_ROAD\",\n",
    "    \"TYP_INT\",\n",
    "    \"URBANICITY\",#was the area urban or rural\n",
    "    \"MAX_SEV\" \n",
    "]\n",
    "\n",
    "veh_cols = [\n",
    "    \"VSPD_LIM\", #speed limit of the area\n",
    "    \"VSURCOND\",\n",
    "    \"ACC_TYPE\", #type of accident\n",
    "    \"SPEEDREL\", #was accident speeding related\n",
    "    \"DEFORMED\" #extent of damage\n",
    "]\n",
    "\n",
    "person_cols = [\n",
    "    \"AGE\",\n",
    "    \"SEX\",\n",
    "    \"PER_TYP\", #role of the person at time of crash i.e. driver, occupant, etc\n",
    "    \"SEAT_POS\", #where in car person was sitting\n",
    "    \"REST_USE\" #restraint system used i.e. seatbelt, motorcycle helmet, etc\n",
    "]\n",
    "\n",
    "df4 = df3[acc_cols + veh_cols + person_cols]\n",
    "\n",
    "UNKNOWN_MAP = {\n",
    "    #accident csv\n",
    "    \"DAY_WEEK\": {9},\n",
    "    \"HOUR\": {99},\n",
    "    \"HARM_EV\": {99},\n",
    "    \"MAN_COLL\": {99, 98},\n",
    "    \"TYP_INT\": {99, 98},\n",
    "    \"REL_ROAD\": {99, 98},\n",
    "    \"LGT_COND\": {9, 8},\n",
    "    \"WEATHER\": {98, 99},\n",
    "    \n",
    "    #vehicle csv\n",
    "    \"VSPD_LIM\": {98, 99},\n",
    "    \"VSURCOND\": {99},\n",
    "    \"ACC_TYPE\": {5, 10, 16, 33, 43, 49, 53, 63, 67, 75, 85, 91, 99},\n",
    "    \"SPEEDREL\": {8, 9},\n",
    "    \"DEFORMED\": {8,9},\n",
    "    \n",
    "    #person csv\n",
    "    \"AGE\": {998, 999},\n",
    "    \"SEX\": {8, 9},\n",
    "    \"PER_TYP\": {9,  19},\n",
    "    \"SEAT_POS\": {98, 99},\n",
    "    \"REST_USE\": {98, 99}\n",
    "}\n",
    "\n",
    "rows = []\n",
    "n = len(df4)\n",
    "for col in df4.columns:\n",
    "    codes = UNKNOWN_MAP.get(col, set())\n",
    "    if len(codes) == 0:\n",
    "        unk_count = 0\n",
    "    else:\n",
    "        unk_count = df4[col].isin(codes).sum()\n",
    "    rows.append({\n",
    "        \"column\": col,\n",
    "        \"unknown_codes\": sorted(codes),\n",
    "        \"unknown_count\": int(unk_count),\n",
    "        \"unknown_percent\": round(100 * unk_count / n, 3) if n else 0.0\n",
    "    })\n",
    "unknown_summary = pd.DataFrame(rows).sort_values(\"unknown_percent\", ascending=False)\n",
    "print(unknown_summary)\n",
    "df_clean = df4.copy()\n",
    "for col, codes in UNKNOWN_MAP.items():\n",
    "    if codes:\n",
    "        df_clean.loc[df_clean[col].isin(codes), col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eef94edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MONTH  DAY_WEEK   HOUR  WEATHER  LGT_COND  HARM_EV  MAN_COLL  REL_ROAD  \\\n",
      "0      False     False  False    False     False    False     False     False   \n",
      "1      False     False  False    False     False    False     False     False   \n",
      "2      False     False  False    False     False    False     False     False   \n",
      "3      False     False  False    False     False    False     False     False   \n",
      "4      False     False  False    False     False    False     False     False   \n",
      "...      ...       ...    ...      ...       ...      ...       ...       ...   \n",
      "49058  False     False  False    False     False    False     False     False   \n",
      "49059  False     False  False    False     False    False     False     False   \n",
      "49060  False     False  False    False     False    False     False     False   \n",
      "49061  False     False  False    False     False    False     False     False   \n",
      "49062  False     False  False    False     False    False     False     False   \n",
      "\n",
      "       TYP_INT  URBANICITY  ...  VSPD_LIM  VSURCOND  ACC_TYPE  SPEEDREL  \\\n",
      "0        False       False  ...     False     False     False     False   \n",
      "1        False       False  ...     False     False     False     False   \n",
      "2        False       False  ...     False     False     False     False   \n",
      "3        False       False  ...     False     False     False     False   \n",
      "4        False       False  ...     False     False     False     False   \n",
      "...        ...         ...  ...       ...       ...       ...       ...   \n",
      "49058    False       False  ...     False     False     False     False   \n",
      "49059    False       False  ...     False     False     False     False   \n",
      "49060    False       False  ...     False     False     False     False   \n",
      "49061    False       False  ...     False     False     False     False   \n",
      "49062    False       False  ...     False     False     False     False   \n",
      "\n",
      "       DEFORMED    AGE    SEX  PER_TYP  SEAT_POS  REST_USE  \n",
      "0         False  False  False    False     False     False  \n",
      "1         False  False  False    False     False     False  \n",
      "2         False  False  False    False     False     False  \n",
      "3         False  False  False    False     False     False  \n",
      "4         False  False  False    False     False     False  \n",
      "...         ...    ...    ...      ...       ...       ...  \n",
      "49058     False  False  False    False     False     False  \n",
      "49059     False  False  False    False     False     False  \n",
      "49060     False  False  False    False     False     False  \n",
      "49061     False  False  False    False     False     False  \n",
      "49062     False  False  False    False     False     False  \n",
      "\n",
      "[49063 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#drop rows where NaN vals > 50% of the row\n",
    "#for remaining rows, impute missing NaN vals\n",
    "df_drop = df_clean.dropna(thresh=0.5)\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(df_drop),\n",
    "    columns=df_drop.columns\n",
    ")\n",
    "print(df_imputed.isnull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c064b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.5499694272708744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.89      0.75      7157\n",
      "         1.0       0.32      0.17      0.22      2984\n",
      "         2.0       0.37      0.32      0.34      2591\n",
      "         3.0       0.36      0.23      0.28      1648\n",
      "         4.0       0.00      0.00      0.00       339\n",
      "\n",
      "    accuracy                           0.55     14719\n",
      "   macro avg       0.34      0.32      0.32     14719\n",
      "weighted avg       0.48      0.55      0.50     14719\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15013\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\15013\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\15013\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "#starting on the model\n",
    "#final accuracy around 54% for only one year of data, around 49000 points\n",
    "#hopefully would improve further with full dataset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X = df_imputed.drop(columns=['MAX_SEV'])\n",
    "y = df_imputed['MAX_SEV']\n",
    "for col in X.columns:\n",
    "    X[col] = X[col].astype('category').cat.codes\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=20, min_samples_split= 200, random_state=RANDOM_STATE)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
