{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f7adf6",
   "metadata": {},
   "source": [
    "want to try and optimize feature selection\n",
    "\n",
    "from what i can see, accident, vehicle, and person datasets seem the most relevant, might try to take some features from there\n",
    "\n",
    "converting each csv into a dataframe for better handling\n",
    "- df is our base df, will be writing it to a new csv when done\n",
    "- finding intersection btwn all three csvs, using that to drop features from df1 and df2.\n",
    "- i then will be adding df1 and df2 (with only unique cols) into df\n",
    "- from here, determine features with strongest correlation to target since our max is 20 features\n",
    "- going to run decision tree model from demo.ipynb on new csv to see if any improvements in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7e32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_18008\\1782660601.py:10: DtypeWarning: Columns (57,59,69,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\vehicle23.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "RANDOM_STATE = 42\n",
    "files = [\"accident23\", \"person23\", \"vehicle23\"]\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\accident23.csv\")\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\vehicle23.csv\")\n",
    "df2 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\person23.csv\")\n",
    "\n",
    "df3 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d257ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50103\n",
      "87461\n",
      "122388\n"
     ]
    }
   ],
   "source": [
    "accidents_col = list(df.columns)\n",
    "vehicles_col = list(df1.columns)\n",
    "person_col = list(df2.columns)\n",
    "\n",
    "intersect = []\n",
    "\n",
    "for x in accidents_col: #there has to be a better way to do this.. \n",
    "    if x in vehicles_col:\n",
    "        if x in person_col:\n",
    "            intersect.append(x) #if feature is in all 3 dfs\n",
    "\n",
    "intersect.remove(\"CASENUM\") #we wanna keep casenum since this is our joining feature\n",
    "#gonna use this list to drop features from df1 and df2\n",
    "\n",
    "df1 = df1.drop(columns = intersect, errors = \"ignore\")\n",
    "df2 = df2.drop(columns = intersect, errors = \"ignore\")\n",
    "\n",
    "print(len(df))\n",
    "print(len(df1))\n",
    "print(len(df2)) #each file has a differnt num of entries, we need to match case numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad91379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all three dfs based on casenum\n",
    "df3 = (\n",
    "    df\n",
    "    .merge(df1, on=\"CASENUM\", how=\"left\")\n",
    "    .merge(df2, on=\"CASENUM\", how=\"left\")\n",
    ")\n",
    "\n",
    "df3 = df3[df3['MAX_SEV'] <= 4] #drop where sev vals > 4\n",
    "\n",
    "\n",
    "#so by this point what we have is df3, which is a dataframe consisting of\n",
    "#each unique column in the three csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d182ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for missing/unknown values \n",
    "#most of the code taken from demo.ipynb\n",
    "\n",
    "#tentative colnames from each csv\n",
    "acc_cols = [\n",
    "    \"MONTH\",\n",
    "    \"DAY_WEEK\",\n",
    "    \"HOUR\",\n",
    "    \"WEATHER\",\n",
    "    \"LGT_COND\",\n",
    "    \"HARM_EV\", #first harmful event that took place\n",
    "    \"MAN_COLL\", #the kind of collision\n",
    "    \"REL_ROAD\",\n",
    "    \"TYP_INT\",\n",
    "    \"URBANICITY\" #was the area urban or rural\n",
    "]\n",
    "\n",
    "veh_cols = [\n",
    "    \"VSPD_LIM\", #speed limit of the area\n",
    "    \"VSURCOND\",\n",
    "    \"ACC_TYPE\", #type of accident\n",
    "    \"SPEEDREL\", #was accident speeding related\n",
    "    \"DEFORMED\" #extent of damage\n",
    "]\n",
    "\n",
    "person_cols = [\n",
    "    \"AGE\",\n",
    "    \"SEX\",\n",
    "    \"PER_TYPE\", #role of the person at time of crash i.e. driver, occupant, etc\n",
    "    \"SEAT_POS\", #where in car person was sitting\n",
    "    \"REST_USE\" #restraint system used i.e. seatbelt, motorcycle helmet, etc\n",
    "]\n",
    "df_clean = df3[[acc_cols + veh_cols + person_col]]\n",
    "\n",
    "UNKNOWN_MAP = {\n",
    "    #accident csv\n",
    "    \"DAY_WEEK\": {9},\n",
    "    \"HOUR\": {99},\n",
    "    \"HARM_EV\": {99},\n",
    "    \"MAN_COLL\": {99, 98},\n",
    "    \"TYP_INT\": {99, 98},\n",
    "    \"REL_ROAD\": {99, 98},\n",
    "    \"LGT_COND\": {9, 8},\n",
    "    \"WEATHER\": {98, 99},\n",
    "    \n",
    "    #vehicle csv\n",
    "    \"VSPD_LIM\": {98, 99},\n",
    "    \"VSURCOND\": {99},\n",
    "    \"ACC_TYPE\": {5, 10, 16, 33, 43, 49, 53, 63, 67, 75, 85, 91, 99},\n",
    "    \"SPEEDREL\": {8, 9},\n",
    "    \"DEFORMED\": {8,9},\n",
    "    \n",
    "    #person csv\n",
    "    \"AGE\": {998, 999},\n",
    "    \"SEX\": {8, 9},\n",
    "    \"PER_TYPE\": {9,  19},\n",
    "    \"SEAT_POS\": {98, 99},\n",
    "    \"REST_USE\": {98, 99}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "n = len(df3)\n",
    "for col in df3.columns:\n",
    "    codes = UNKNOWN_MAP.get(col, set())\n",
    "    if len(codes) == 0:\n",
    "        unk_count = 0\n",
    "    else:\n",
    "        unk_count = df3[col].isin(codes).sum()\n",
    "    rows.append({\n",
    "        \"column\": col,\n",
    "        \"unknown_codes\": sorted(codes),\n",
    "        \"unknown_count\": int(unk_count),\n",
    "        \"unknown_percent\": round(100 * unk_count / n, 3) if n else 0.0\n",
    "    })\n",
    "unknown_summary = pd.DataFrame(rows).sort_values(\"unknown_percent\", ascending=False)\n",
    "print(unknown_summary)\n",
    "df_clean = df3.copy()\n",
    "for col, codes in UNKNOWN_MAP.items():\n",
    "    if codes:\n",
    "        df_clean.loc[df_clean[col].isin(codes), col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c064b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting on the model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X = df_clean.drop(columns=['MAX_SEV'])\n",
    "y = df_clean['MAX_SEV']\n",
    "for col in X.columns:\n",
    "    X[col] = X[col].astype('category').cat.codes\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=10, min_samples_split= 200, random_state=RANDOM_STATE)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
