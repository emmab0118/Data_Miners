{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f7adf6",
   "metadata": {},
   "source": [
    "want to try and optimize feature selection\n",
    "\n",
    "from what i can see, accident, vehicle, and person datasets seem the most relevant, might try to take some features from there\n",
    "\n",
    "converting each csv into a dataframe for better handling\n",
    "- df is our base df, will be writing it to a new csv when done\n",
    "- finding intersection btwn all three csvs, using that to drop features from df1 and df2.\n",
    "- i then will be adding df1 and df2 (with only unique cols) into df\n",
    "- from here, determine features with strongest correlation to target since our max is 20 features\n",
    "- going to run decision tree model from demo.ipynb on new csv to see if any improvements in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7e32f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\accident23.csv\")\\ndf1 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\vehicle23.csv\")\\ndf2 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\person23.csv\")\\n\\ndf3 = pd.DataFrame()'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\"\"\"df = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\accident23.csv\")\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\vehicle23.csv\")\n",
    "df2 = pd.read_csv(\"C:\\\\Users\\\\15013\\\\Desktop\\\\CRSS2023CSV\\\\person23.csv\")\n",
    "\n",
    "df3 = pd.DataFrame()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d257ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50103\n",
      "87461\n",
      "122388\n"
     ]
    }
   ],
   "source": [
    "accidents_col = list(df.columns)\n",
    "vehicles_col = list(df1.columns)\n",
    "person_col = list(df2.columns)\n",
    "\n",
    "intersect = []\n",
    "\n",
    "for x in accidents_col: #there has to be a better way to do this.. \n",
    "    if x in vehicles_col:\n",
    "        if x in person_col:\n",
    "            intersect.append(x) #if feature is in all 3 dfs\n",
    "\n",
    "intersect.remove(\"CASENUM\") #we wanna keep casenum since this is our joining feature\n",
    "#gonna use this list to drop features from df1 and df2\n",
    "\n",
    "df1 = df1.drop(columns = intersect, errors = \"ignore\")\n",
    "df2 = df2.drop(columns = intersect, errors = \"ignore\")\n",
    "\n",
    "print(len(df))\n",
    "print(len(df1))\n",
    "print(len(df2)) #each file has a differnt num of entries, we need to match case numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad91379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CASENUM', 'PSU', 'PSU_VAR', 'PSUSTRAT', 'REGION', 'REGIONNAME', 'URBANICITY', 'URBANICITYNAME', 'STRATUM', 'STRATUMNAME', 'PJ', 'WEIGHT', 'PEDS', 'PERNOTMVIT', 'VE_TOTAL', 'VE_FORMS', 'PVH_INVL', 'PERMVIT', 'MONTH', 'MONTHNAME', 'DAY_WEEK', 'DAY_WEEKNAME', 'WKDY_IM', 'WKDY_IMNAME', 'YEAR', 'YEARNAME', 'HOUR', 'HOURNAME', 'HOUR_IM', 'HOUR_IMNAME', 'MINUTE', 'MINUTENAME', 'MINUTE_IM', 'MINUTE_IMNAME', 'HARM_EV', 'HARM_EVNAME', 'EVENT1_IM', 'EVENT1_IMNAME', 'MAN_COLL', 'MAN_COLLNAME', 'MANCOL_IM', 'MANCOL_IMNAME', 'RELJCT1', 'RELJCT1NAME', 'RELJCT1_IM', 'RELJCT1_IMNAME', 'RELJCT2', 'RELJCT2NAME', 'RELJCT2_IM', 'RELJCT2_IMNAME', 'TYP_INT', 'TYP_INTNAME', 'REL_ROAD', 'REL_ROADNAME', 'WRK_ZONE', 'WRK_ZONENAME', 'LGT_COND', 'LGT_CONDNAME', 'LGTCON_IM', 'LGTCON_IMNAME', 'WEATHER', 'WEATHERNAME', 'WEATHR_IM', 'WEATHR_IMNAME', 'SCH_BUS_x', 'SCH_BUSNAME_x', 'INT_HWY', 'INT_HWYNAME', 'MAX_SEV', 'MAX_SEVNAME', 'MAXSEV_IM', 'MAXSEV_IMNAME', 'NUM_INJ', 'NUM_INJNAME', 'NO_INJ_IM', 'NO_INJ_IMNAME', 'ALCOHOL', 'ALCOHOLNAME', 'ALCHL_IM', 'ALCHL_IMNAME', 'VEH_NO_x', 'NUMOCCS', 'NUMOCCSNAME', 'UNITTYPE', 'UNITTYPENAME', 'HIT_RUN', 'HIT_RUNNAME', 'VIN', 'VINNAME', 'MOD_YEAR_x', 'MOD_YEARNAME_x', 'MDLYR_IM', 'MDLYR_IMNAME', 'VPICMAKE_x', 'VPICMAKENAME_x', 'VPICMODEL_x', 'VPICMODELNAME_x', 'VPICBODYCLASS_x', 'VPICBODYCLASSNAME_x', 'MAKE_x', 'MAKENAME_x', 'MODEL', 'BODY_TYP_x', 'BODY_TYPNAME_x', 'ICFINALBODY_x', 'ICFINALBODYNAME_x', 'GVWR_FROM_x', 'GVWR_FROMNAME_x', 'GVWR_TO_x', 'GVWR_TONAME_x', 'TOW_VEH_x', 'TOW_VEHNAME_x', 'TRLR1VIN', 'TRLR1VINNAME', 'TRLR2VIN', 'TRLR2VINNAME', 'TRLR3VIN', 'TRLR3VINNAME', 'TRLR1GVWR', 'TRLR1GVWRNAME', 'TRLR2GVWR', 'TRLR2GVWRNAME', 'TRLR3GVWR', 'TRLR3GVWRNAME', 'J_KNIFE', 'J_KNIFENAME', 'MCARR_ID', 'MCARR_IDNAME', 'MCARR_I1', 'MCARR_I1NAME', 'MCARR_I2', 'MCARR_I2NAME', 'V_Config', 'V_ConfigNAME', 'CARGO_BT', 'CARGO_BTNAME', 'HAZ_INV', 'HAZ_INVNAME', 'HAZ_PLAC', 'HAZ_PLACNAME', 'HAZ_ID', 'HAZ_IDNAME', 'HAZ_CNO', 'HAZ_CNONAME', 'HAZ_REL', 'HAZ_RELNAME', 'BUS_USE', 'BUS_USENAME', 'SPEC_USE_x', 'SPEC_USENAME_x', 'EMER_USE_x', 'EMER_USENAME_x', 'TRAV_SP', 'TRAV_SPNAME', 'UNDEROVERRIDE', 'UNDEROVERRIDENAME', 'ROLLOVER_x', 'ROLLOVERNAME_x', 'ROLINLOC', 'ROLINLOCNAME', 'IMPACT1_x', 'IMPACT1NAME_x', 'IMPACT1_IM', 'IMPACT1_IMNAME', 'DEFORMED', 'DEFORMEDNAME', 'TOWED', 'TOWEDNAME', 'M_HARM', 'M_HARMNAME', 'VEVENT_IM', 'VEVENT_IMNAME', 'FIRE_EXP_x', 'FIRE_EXPNAME_x', 'DR_PRES', 'DR_PRESNAME', 'MAX_VSEV', 'MAX_VSEVNAME', 'MXVSEV_IM', 'MXVSEV_IMNAME', 'NUM_INJV', 'NUM_INJVNAME', 'NUMINJ_IM', 'NUMINJ_IMNAME', 'VEH_ALCH', 'VEH_ALCHNAME', 'V_ALCH_IM', 'V_ALCH_IMNAME', 'MAK_MOD_x', 'MAK_MODNAME_x', 'DR_ZIP', 'DR_ZIPNAME', 'SPEEDREL', 'SPEEDRELNAME', 'VTRAFWAY', 'VTRAFWAYNAME', 'VNUM_LAN', 'VNUM_LANNAME', 'VSPD_LIM', 'VSPD_LIMNAME', 'VALIGN', 'VALIGNNAME', 'VPROFILE', 'VPROFILENAME', 'VSURCOND', 'VSURCONDNAME', 'VTRAFCON', 'VTRAFCONNAME', 'VTCONT_F', 'VTCONT_FNAME', 'P_CRASH1', 'P_CRASH1NAME', 'PCRASH1_IM', 'PCRASH1_IMNAME', 'P_CRASH2', 'P_CRASH2NAME', 'P_CRASH3', 'P_CRASH3NAME', 'PCRASH4', 'PCRASH4NAME', 'PCRASH5', 'PCRASH5NAME', 'ACC_TYPE', 'ACC_TYPENAME', 'ACC_CONFIG', 'ACC_CONFIGNAME', 'VEH_NO_y', 'PER_NO', 'SCH_BUS_y', 'SCH_BUSNAME_y', 'MOD_YEAR_y', 'MOD_YEARNAME_y', 'VPICMAKE_y', 'VPICMAKENAME_y', 'VPICMODEL_y', 'VPICMODELNAME_y', 'VPICBODYCLASS_y', 'VPICBODYCLASSNAME_y', 'MAKE_y', 'MAKENAME_y', 'BODY_TYP_y', 'BODY_TYPNAME_y', 'ICFINALBODY_y', 'ICFINALBODYNAME_y', 'GVWR_FROM_y', 'GVWR_FROMNAME_y', 'GVWR_TO_y', 'GVWR_TONAME_y', 'TOW_VEH_y', 'TOW_VEHNAME_y', 'SPEC_USE_y', 'SPEC_USENAME_y', 'EMER_USE_y', 'EMER_USENAME_y', 'ROLLOVER_y', 'ROLLOVERNAME_y', 'IMPACT1_y', 'IMPACT1NAME_y', 'FIRE_EXP_y', 'FIRE_EXPNAME_y', 'MAK_MOD_y', 'MAK_MODNAME_y', 'AGE', 'AGENAME', 'AGE_IM', 'AGE_IMNAME', 'SEX', 'SEXNAME', 'SEX_IM', 'SEX_IMNAME', 'PER_TYP', 'PER_TYPNAME', 'DEVTYPE', 'DEVTYPENAME', 'DEVMOTOR', 'DEVMOTORNAME', 'INJ_SEV', 'INJ_SEVNAME', 'INJSEV_IM', 'INJSEV_IMNAME', 'SEAT_POS', 'SEAT_POSNAME', 'SEAT_IM', 'SEAT_IMNAME', 'REST_USE', 'REST_USENAME', 'REST_MIS', 'REST_MISNAME', 'HELM_USE', 'HELM_USENAME', 'HELM_MIS', 'HELM_MISNAME', 'AIR_BAG', 'AIR_BAGNAME', 'EJECTION', 'EJECTIONNAME', 'EJECT_IM', 'EJECT_IMNAME', 'DRINKING', 'DRINKINGNAME', 'PERALCH_IM', 'PERALCH_IMNAME', 'ALC_STATUS', 'ALC_STATUSNAME', 'ATST_TYP', 'ATST_TYPNAME', 'ALC_RES', 'ALC_RESNAME', 'DRUGS', 'DRUGSNAME', 'HOSPITAL', 'HOSPITALNAME', 'STR_VEH', 'LOCATION', 'LOCATIONNAME']\n",
      "49063\n"
     ]
    }
   ],
   "source": [
    "#merge all three dfs based on casenum\n",
    "\n",
    "#when i first ran all my code the df len was 500,000\n",
    "#assuming this is because of multiple people per car in accident?\n",
    "# if we remove these two lines then we will have much more data to work with\n",
    "df1_first = df1.groupby(\"CASENUM\").head(1)\n",
    "df2_first = df2.groupby(\"CASENUM\").head(1)\n",
    "\n",
    "df3 = (\n",
    "    df\n",
    "    .merge(df1_first, on=\"CASENUM\", how=\"left\")\n",
    "    .merge(df2_first, on=\"CASENUM\", how=\"left\")\n",
    ")\n",
    "\n",
    "\"\"\"df3 = (\n",
    "    df\n",
    "    .merge(df1, on=\"CASENUM\", how=\"left\")\n",
    "    .merge(df2, on=\"CASENUM\", how=\"left\")\n",
    ")\"\"\"\n",
    "\n",
    "\n",
    "df3 = df3[df3['MAX_SEV'] <= 4] #drop where sev vals > 4\n",
    "print(list(df3.columns))\n",
    "\n",
    "#so by this point what we have is df3, which is a dataframe consisting of\n",
    "#each unique column in the three csvs\n",
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d182ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for missing/unknown values and changing to NaN\n",
    "#most of the code taken from demo.ipynb\n",
    "\n",
    "#tentative colnames from each csv\n",
    "acc_cols = [\n",
    "    \"CASENUM\",\n",
    "    \"MONTH\",\n",
    "    \"DAY_WEEK\",\n",
    "    \"HOUR\",\n",
    "    \"WEATHER\",\n",
    "    \"LGT_COND\",\n",
    "    \"HARM_EV\", #first harmful event that took place\n",
    "    \"MAN_COLL\", #the kind of collision\n",
    "    \"REL_ROAD\",\n",
    "    \"TYP_INT\",\n",
    "    \"URBANICITY\",#was the area urban or rural\n",
    "    \"MAX_SEV\" \n",
    "]\n",
    "\n",
    "veh_cols = [\n",
    "    \"CASENUM\",\n",
    "    \"VSPD_LIM\", #speed limit of the area\n",
    "    \"VSURCOND\",\n",
    "    \"ACC_TYPE\", #type of accident\n",
    "    \"SPEEDREL\", #was accident speeding related\n",
    "    \"DEFORMED\" #extent of damage\n",
    "]\n",
    "\n",
    "person_cols = [\n",
    "    \"CASENUM\",\n",
    "    \"AGE\",\n",
    "    \"SEX\",\n",
    "    \"PER_TYP\", #role of the person at time of crash i.e. driver, occupant, etc\n",
    "    \"SEAT_POS\", #where in car person was sitting\n",
    "    \"REST_USE\" #restraint system used i.e. seatbelt, motorcycle helmet, etc\n",
    "]\n",
    "\n",
    "#went through the manual, found what values represent NaN for each colname\n",
    "UNKNOWN_MAP = {\n",
    "    #accident csv\n",
    "    \"DAY_WEEK\": {9},\n",
    "    \"HOUR\": {99},\n",
    "    \"HARM_EV\": {99},\n",
    "    \"MAN_COLL\": {99, 98},\n",
    "    \"TYP_INT\": {99, 98},\n",
    "    \"REL_ROAD\": {99, 98},\n",
    "    \"LGT_COND\": {9, 8},\n",
    "    \"WEATHER\": {98, 99},\n",
    "    \n",
    "    #vehicle csv\n",
    "    \"VSPD_LIM\": {98, 99},\n",
    "    \"VSURCOND\": {99},\n",
    "    \"ACC_TYPE\": {5, 10, 16, 33, 43, 49, 53, 63, 67, 75, 85, 91, 99},\n",
    "    \"SPEEDREL\": {8, 9},\n",
    "    \"DEFORMED\": {8,9},\n",
    "    \n",
    "    #person csv\n",
    "    \"AGE\": {998, 999},\n",
    "    \"SEX\": {8, 9},\n",
    "    \"PER_TYP\": {9,  19},\n",
    "    \"SEAT_POS\": {98, 99},\n",
    "    \"REST_USE\": {98, 99}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef94edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (23,24,70,71) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (26,27,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (26,27,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:7: DtypeWarning: Columns (63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  acc_df = pd.read_csv(acc, encoding= enc)[acc_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (45,47,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (45,47,127) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (55,57,69,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (55,57,69,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
      "C:\\Users\\15013\\AppData\\Local\\Temp\\ipykernel_13000\\1413745174.py:9: DtypeWarning: Columns (57,59,69,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n"
     ]
    }
   ],
   "source": [
    "#feed 3 csvs into combine, feed combine() into clean_nan, feed clean_nan() into imp\n",
    "\n",
    "def combine(acc, per, veh):\n",
    "    enc = \"cp1252\"\n",
    "    #take in accident, person, vehicle csvs for a year, join them together\n",
    "    #using the defined columns in prev cell, then use pd.merge\n",
    "    acc_df = pd.read_csv(acc, encoding= enc)[acc_cols]\n",
    "    per_df = pd.read_csv(per, encoding = enc)[person_cols]\n",
    "    veh_df = pd.read_csv(veh, encoding = enc )[veh_cols]\n",
    "    \n",
    "    #first record per case, when i didnt do this each df had like 500k entries\n",
    "    acc_first = acc_df.groupby(\"CASENUM\").head(1)\n",
    "    per_first = per_df.groupby(\"CASENUM\").head(1)\n",
    "    veh_first = veh_df.groupby(\"CASENUM\").head(1)\n",
    "\n",
    "    final = ( #merge the frames based on case number\n",
    "        acc_first\n",
    "        .merge(per_first, on=\"CASENUM\", how=\"left\")\n",
    "        .merge(veh_first, on=\"CASENUM\", how=\"left\")\n",
    "    )\n",
    "    final = final[final['MAX_SEV'] <= 4] #drop where sev vals > 4\n",
    "\n",
    "    return final\n",
    "    \n",
    "\n",
    "def clean_nan(frame):\n",
    "    df_clean = frame.copy()\n",
    "\n",
    "    rows = []\n",
    "    n = len(df_clean)\n",
    "\n",
    "    # convert all \"unknown codes\" to NaN using the map\n",
    "    for col, codes in UNKNOWN_MAP.items():\n",
    "        if len(codes) > 0 and col in df_clean.columns:\n",
    "            df_clean.loc[df_clean[col].isin(codes), col] = np.nan\n",
    "\n",
    "        unk_count = df_clean[col].isna().sum() if codes else 0\n",
    "\n",
    "        rows.append({\n",
    "            \"column\": col,\n",
    "            \"unknown_codes\": sorted(codes),\n",
    "            \"unknown_count\": int(unk_count),\n",
    "            \"unknown_percent\": round(100 * unk_count / n, 3) if n else 0.0\n",
    "        })\n",
    "\n",
    "    #unknown_summary = pd.DataFrame(rows).sort_values(\"unknown_percent\", ascending=False)\n",
    "    #print(unknown_summary)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "#drop rows where NaN vals > 50% of the row\n",
    "#for remaining rows, impute missing NaN vals\n",
    "\n",
    "def imp(frame):\n",
    "    min_non_na = int(0.7 * len(frame.columns))\n",
    "    df_drop = frame.dropna(thresh=min_non_na)\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    df_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df_drop),\n",
    "        columns=df_drop.columns\n",
    "    )\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "workspace = \"C:\\\\Users\\\\15013\\\\Desktop\\\\acc_per_veh\\\\\"\n",
    "files = [[\"accident16.CSV\",'person16.CSV','vehicle16.CSV'], [\"accident17.CSV\",'person17.CSV','vehicle17.CSV'], [\"accident18.csv\",'person18.csv','vehicle18.csv'], [\"accident19.CSV\",'person19.CSV','vehicle19.csv'], \n",
    "        [\"accident20.csv\",'person20.csv','vehicle20.csv'], [\"accident21.csv\",'person21.csv','vehicle21.csv'], [\"accident22.csv\",'person22.csv','vehicle22.csv'], [\"accident23.csv\",'person23.csv','vehicle23.csv']]\n",
    "\n",
    "final = pd.DataFrame()\n",
    "for f in files:\n",
    "    d1 = combine(workspace+f[0], workspace+f[1], workspace+f[2])\n",
    "    d2 = clean_nan(d1)\n",
    "    d3 = imp(d2)\n",
    "    final = pd.concat([final, d3], ignore_index=True)\n",
    "\n",
    "final.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c064b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.5499694272708744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.89      0.75      7157\n",
      "         1.0       0.32      0.17      0.22      2984\n",
      "         2.0       0.37      0.32      0.34      2591\n",
      "         3.0       0.36      0.23      0.28      1648\n",
      "         4.0       0.00      0.00      0.00       339\n",
      "\n",
      "    accuracy                           0.55     14719\n",
      "   macro avg       0.34      0.32      0.32     14719\n",
      "weighted avg       0.48      0.55      0.50     14719\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15013\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\15013\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\15013\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "#starting on the model\n",
    "#final accuracy around 54% for only one year of data, around 49000 points\n",
    "#hopefully would improve further with full dataset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X = df_imputed.drop(columns=['MAX_SEV'])\n",
    "y = df_imputed['MAX_SEV']\n",
    "for col in X.columns:\n",
    "    X[col] = X[col].astype('category').cat.codes\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=20, min_samples_split= 200, random_state=RANDOM_STATE)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
